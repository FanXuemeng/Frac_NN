"""
@author: Maziar Raissi
"""

import torch
import torch.nn as nn
import timeit
import nodepy.linear_multistep_method as lm
import numpy as np
from scipy.integrate import odeint

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from plotting import newfig, savefig
import matplotlib.gridspec as gridspec

np.random.seed(1234)



def colorline3d(ax, x, y, z, cmap):
    N = len(x)
    skip = int(0.01 * N)
    for i in range(0, N, skip):
        ax.plot(x[i:i + skip + 1], y[i:i + skip + 1], z[i:i + skip + 1], color=cmap(int(255 * i / N)))

class Net(nn.Module):
    def __init__(self, layers):
        super().__init__()
        # 定义三层全连接层
        self.linear1 = nn.Linear(layers[0], layers[1])
        self.linear2 = nn.Linear(layers[1], layers[2])

    def forward(self, x):
        x = torch.tanh(self.linear1(x))
        return self.linear2(x)


def net_F(NN, X):  # S x (N-M+1) x D
    # X_reshaped = torch.reshape(X, [-1, self.D])  # S(N-M+1) x D
    F = NN.forward(X)  # S(N-M+1) x D
    # F = torch.reshape(F_reshaped, [self.S, -1, self.D])  # S x (N-M+1) x D
    return F  # S x (N-M+1) x D


def net_Y(NN, alpha, beta, X):  # S x N x D
    Y = alpha[0] * X[M:, :] + dt * beta[0] * NN.forward(X[M:, :])
    for m in range(1, M + 1):
        Y = Y + alpha[m] * X[M - m:-m, :] + dt * beta[m] * NN.forward(X[M - m:-m, :])  # S x (N-M+1) x D
    return Y # S x (N-M+1) x D


if __name__ == "__main__":

    # function that returns dx/dt
    def f(x, t):  # x is 3 x 1
        sigma = 10.0
        beta = 8.0 / 3.0
        rho = 28.0
        if len(np.shape(x)) == 2:
            f1 = sigma * (x[:, 1] - x[:, 0])
            f2 = x[:, 0] * (rho - x[:, 2]) - x[:, 1]
            f3 = x[:, 0] * x[:, 1] - beta * x[:, 2]
            f = np.transpose(np.array([f1, f2, f3]))
        else:
            f1 = sigma * (x[1] - x[0])
            f2 = x[0] * (rho - x[2]) - x[1]
            f3 = x[0] * x[1] - beta * x[2]
            f = np.array([f1, f2, f3])
        return f


    # time points
    t_star = np.arange(0, 25, 0.01)

    # initial condition
    x0 = np.array([-8.0, 7.0, 27])

    # solve ODE
    X_star = odeint(f, x0, t_star)

    noise = 0.00

    skip = 1
    dt = t_star[skip] - t_star[0]
    X_train = X_star[0::skip, :]
    X_train = X_train + noise * X_train.std(0) * np.random.randn(X_train.shape[0], X_train.shape[1])

    X_train = np.reshape(X_train, (1, X_train.shape[0], X_train.shape[1]))

    layers = [3, 256, 3]

    M = 1
    scheme = 'AM'
    # Load weights
    switch = {'AM': lm.Adams_Moulton,
              'AB': lm.Adams_Bashforth,
              'BDF': lm.backward_difference_formula}
    method = switch[scheme](M)
    alpha = np.float32(-method.alpha[::-1])
    beta = np.float32(method.beta[::-1])

    X = X_train
    S = X.shape[0]  # number of trajectories
    N = X.shape[1]  # number of time snapshots
    D = X.shape[2]  # number of dimensions
    # X = torch.reshape(torch.tensor(X.astype(np.float32)), [-1, D])  # S x N x D
    X = torch.reshape(torch.from_numpy(X).type(torch.FloatTensor), [-1, D])  # S x N x D

    M = M  # number of Adams-Moulton steps

    # layers = layers
    # device = torch.device("cuda")

    # X_tr = torch.empty(S, 0, D)  # S x N x D
    # X_star_tr = torch.empty(0, D)  # N_star x D

    scope_name = str(np.random.randint(1e6))
    NN = Net(layers)

    learning_rate = 0.001
    optimizer_Adam = torch.optim.Adam(NN.parameters(), lr=learning_rate)
    # loss_func = torch.nn.MSELoss(reduction="mean")
#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer_Adam, step_size=5000, gamma=0.9)

    # NN.load_state_dict(torch.load("NN.pkl"))
    N_Iter = 30000
    start_time = timeit.default_timer()
    for it in range(N_Iter):
        # Y_pred =   # S x N x D
        loss = D*torch.mean(torch.square(net_Y(NN,alpha,beta,X)))
            #torch.sqrt( torch.square(net_Y(NN,X)).sum())  # self.loss_func(self.net_Y(self.X), torch.zeros(self.N-1, self.D))
        optimizer_Adam.zero_grad()
        loss.backward()
        optimizer_Adam.step()
#         scheduler.step()

        # Print
        if it % 100 == 0:
            elapsed = timeit.default_timer() - start_time
            loss_value = loss  # (torch.tensor(self.X.astype(np.float32)))
            print('sIt: %d, Loss: %.3e, Time: %.2f' %
                  (it, loss_value, elapsed))
            print(optimizer_Adam.state_dict()['param_groups'][0]['lr'])

            # for name, parms in NN.named_parameters():
            #     print('-->name:', name)
            #     print('-->para:', parms)
            #     print('-->grad_requirs:', parms.requires_grad)
            #     print('-->grad_value:', parms.grad)
            #     print("===")
            # print(optimizer_Adam)

            start_time = timeit.default_timer()
            # print("current fnn:", self.net_F(self.X))
#             print("current accnn:", net_Y(NN,alpha,beta, X))

    torch.save(NN.state_dict(), "NN.pkl")


    def learned_f(x,t):
        f = NN.forward(torch.tensor(x.astype(np.float32))).detach().numpy()
        return f

    learned_X_star = odeint(learned_f, x0, t_star)

    ####### Plotting ##################
    fig, ax = newfig(1.0, 0.8)
    ax.axis('off')

    gs0 = gridspec.GridSpec(1, 2)
    gs0.update(top=0.95, bottom=0.1, left=0.0, right=0.90, wspace=0.15)

    ax = plt.subplot(gs0[:, 0:1], projection='3d')
    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    colorline3d(ax, X_star[:, 0], X_star[:, 1], X_star[:, 2], cmap=plt.cm.ocean)
    ax.grid(False)
    ax.set_xlim([-20, 20])
    ax.set_ylim([-50, 50])
    ax.set_zlim([0, 50])
    ax.set_xticks([-20, 0, 20])
    ax.set_yticks([-40, 0, 40])
    ax.set_zticks([0, 25, 50])
    ax.set_xlabel('$x$')
    ax.set_ylabel('$y$')
    ax.set_zlabel('$z$')
    ax.set_title('Exact Dynamics', fontsize=10)

    ax = plt.subplot(gs0[:, 1:2], projection='3d')
    ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    ax.w_zaxis.set_pane_color((1.0, 1.0, 1.0, 1.0))
    colorline3d(ax, learned_X_star[:, 0], learned_X_star[:, 1], learned_X_star[:, 2], cmap=plt.cm.ocean)
    ax.grid(False)
    ax.set_xlim([-20, 20])
    ax.set_ylim([-50, 50])
    ax.set_zlim([0, 50])
    ax.set_xticks([-20, 0, 20])
    ax.set_yticks([-40, 0, 40])
    ax.set_zticks([0, 25, 50])
    ax.set_xlabel('$x$')
    ax.set_ylabel('$y$')
    ax.set_zlabel('$z$')
    ax.set_title('Learned Dynamics', fontsize=10)

    savefig('./figures/Lorenz1', crop=False)

    ####### Plotting ##################

    fig, ax = newfig(1.0, 1.5)
    ax.axis('off')

    gs0 = gridspec.GridSpec(3, 1)
    gs0.update(top=0.95, bottom=0.15, left=0.1, right=0.95, hspace=0.5)

    ax = plt.subplot(gs0[0:1, 0:1])
    ax.plot(t_star, X_star[:, 0], 'r-')
    ax.plot(t_star, learned_X_star[:, 0], 'k--')
    ax.set_xlabel('$t$')
    ax.set_ylabel('$x$')

    ax = plt.subplot(gs0[1:2, 0:1])
    ax.plot(t_star, X_star[:, 1], 'r-')
    ax.plot(t_star, learned_X_star[:, 1], 'k--')
    ax.set_xlabel('$t$')
    ax.set_ylabel('$y$')

    ax = plt.subplot(gs0[2:3, 0:1])
    ax.plot(t_star, X_star[:, 2], 'r-', label='Exact Dynamics')
    ax.plot(t_star, learned_X_star[:, 2], 'k--', label='Learned Dynamics')
    ax.set_xlabel('$t$')
    ax.set_ylabel('$z$')
    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.4), ncol=2, frameon=False)

    savefig('./figures/Lorenz_Traj1', crop=False)



